{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***multilingual reverse dictionary dataset generation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import threading\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "data_src_file = 'data_train.json'\n",
    "data_target_file = 'train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 5\n",
    "lang_size = 20000\n",
    "langs = ('pt', 'it', 'zh-cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = json.load(open(os.path.join(data_dir, data_src_file), 'r', encoding='utf8'))\n",
    "defs_en = list(map(lambda x: x['definitions'], data_en))\n",
    "data_en_size = len(data_en)\n",
    "assert data_en_size == len(defs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_samples = {}\n",
    "for lang in langs:\n",
    "    lang_samples[lang] = random.choices(data_en, k=lang_size)\n",
    "    assert len(lang_samples[lang]) == lang_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(name, lang):\n",
    "    data_lang = []\n",
    "    print(f'\\tstarting: {name} {lang}')\n",
    "    for i in range(0, lang_size, bulk_size):\n",
    "        lang_samples_bulk = lang_samples[lang][i:i+bulk_size]\n",
    "        lang_defs_bulk = list(map(lambda x: x['definitions'], lang_samples_bulk))\n",
    "        try: lang_defs_bulk_trans = list(map(lambda x: x.text, translator.translate(lang_defs_bulk, src='en', dest=lang)))\n",
    "        except: continue\n",
    "        lang_samples_bulk_trans = copy.deepcopy(lang_samples_bulk)\n",
    "        for j in range(bulk_size):\n",
    "            lang_samples_bulk_trans[j]['definitions'] = lang_defs_bulk_trans[j]\n",
    "        data_lang.extend(lang_samples_bulk_trans)\n",
    "        json.dump(data_lang, open(lang + '-' + data_target_file, 'w', encoding='utf8'), ensure_ascii=False, indent=4)\n",
    "    print(f'\\tfinishing: {name} {lang}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = list()\n",
    "for i, lang in enumerate(langs):\n",
    "    print(f'Main: create and start thread {i} {lang}')\n",
    "    x = threading.Thread(target=collect, args=(i, lang))\n",
    "    threads.append(x)\n",
    "    x.start()\n",
    "\n",
    "for i, thread in enumerate(threads):\n",
    "    print(f'Main: before joining thread {i} {langs[i]}')\n",
    "    thread.join()\n",
    "    print(f'Main: thread {i} {langs[i]} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***merging data from different languages***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ('pt', 'it', 'zh-cn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge instances from different sources for the same language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilang_dict = {}\n",
    "for lang in langs:\n",
    "    print(f'processing {lang}...')\n",
    "    lang_processed_data, samples_seen, i = [], set(), 0\n",
    "    file_name = f'{lang}{i}-{data_target_file}'\n",
    "    while os.path.exists(file_name):\n",
    "        print('\\t' + file_name)\n",
    "        data = json.load(open(file_name, 'r', encoding='utf8'))\n",
    "        for sample in data:\n",
    "            sample['lang'] = lang\n",
    "            if (sample['word'], sample['definitions']) in samples_seen:\n",
    "                continue\n",
    "            samples_seen.add((sample['word'], sample['definitions']))\n",
    "            lang_processed_data.append(sample)\n",
    "        i += 1\n",
    "        file_name = f'{lang}{i}-{data_target_file}'\n",
    "    print(f'processed {len(lang_processed_data)} {lang} samples')\n",
    "    json.dump(lang_processed_data, open(lang + '-merged-' + data_target_file, 'w', encoding='utf8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging different languages and subsampling english data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_size = 80000\n",
    "data_en_subsample = random.choices(data_en, k=en_size)\n",
    "for sample in data_en_subsample:\n",
    "    sample['lang'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] + data_en_subsample\n",
    "for lang in langs:\n",
    "    print(f'processing {lang}...')\n",
    "    file_name = f'{lang}-merged-{data_target_file}'\n",
    "    data += json.load(open(file_name, 'r', encoding='utf8'))\n",
    "print(f'processed {len(data)} samples')\n",
    "json.dump(data, open(data_target_file, 'w', encoding='utf8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f05847f09a411ebd952eeb5e3719b19d52f615b3ede78b45d695d46bb7e8e51"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
